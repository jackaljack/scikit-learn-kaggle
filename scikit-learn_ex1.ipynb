{
 "metadata": {
  "name": "",
  "signature": "sha256:b2c651f7becc7e4313445c869a94c19c0c13dc706b3c701025374ee264f069ca"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "scikit-learn_ex1"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "import libraries and modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.decomposition.pca import PCA\n",
      "from sklearn import svm\n",
      "from sklearn import preprocessing as pp\n",
      "from sklearn import cross_validation as cv\n",
      "from sklearn import grid_search\n",
      "import matplotlib.pyplot as plt\n",
      "from pylab import pcolor, colorbar, show"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Import data from CSV"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training set (40 variables, 1000 samples) and test set (40 variables, 9000 samples) are already separated into different CSV files. The training labels are stored in a different CSV."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = pd.read_csv(\"data/train.csv\", sep=',', header=None).as_matrix()\n",
      "X_test  = pd.read_csv(\"data/test.csv\", sep=',', header=None).as_matrix()\n",
      "y       = pd.read_csv(\"data/trainLabels.csv\", sep=',', header=None)[0].as_matrix()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exploratory analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Show a **scatterplot** of the first 5 variables, compute the **correlation matrix**. As we can see, correlation is quite low."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scatter plot of training set\n",
      "df = pd.DataFrame(X_train[:, 0:5])\n",
      "pd.tools.plotting.scatter_matrix(df, alpha=0.3, figsize=(8, 8), diagonal='hist')\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# correlation matrix (correlation between 2 variables is very small)\n",
      "R = np.corrcoef(X_train.transpose())\n",
      "plt.figure(2)\n",
      "pcolor(R)\n",
      "colorbar()\n",
      "plt.title(\"correlation matrix\")\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to **reduce the dimensionality** of the problem and obtain a better performance of the classifier, we can perform a PCA. Since the variables are uncorrelated and with variance almost equal to 1, whitening is not necessary. We can plot all the principal components with their relative explained variance (**scree plot**). \n",
      "By centering the variables, principal components remain the same, by standardizing the variables, principal components change. Often it is better to standardize the variables, but in this datasets all the 40 variables have a very similar variance, so we choose to not standardize them. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# center the variables before performing PCA\n",
      "X_train = pp.scale(X_train, with_mean=True, with_std=False)\n",
      "X_test = pp.scale(X_test, with_mean=True, with_std=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca40 = PCA(n_components=40, whiten=False) \n",
      "pca40.fit(X_train)\n",
      "print(pca40.explained_variance_ratio_)\n",
      "\n",
      "# plot all the principal components with their relative explained variance\n",
      "features = [x for x in range(1,41)]\n",
      "plt.figure(3)\n",
      "plt.plot(features, pca40.explained_variance_ratio_, 'g--', marker='o')\n",
      "plt.axis([1, 40, 0, 0.3])\n",
      "plt.grid(True)\n",
      "plt.xlabel(\"principal components\"), plt.ylabel(\"variance explained\")\n",
      "plt.title(\"scree plot\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  2.50544034e-01   2.05504804e-01   8.02647324e-02   5.03365785e-02\n",
        "   4.89595071e-02   4.48990382e-02   4.17078061e-02   3.12793704e-02\n",
        "   2.30982735e-02   2.10016623e-02   1.61972964e-02   1.26992466e-02\n",
        "   8.85167569e-03   8.46121664e-03   8.27947080e-03   8.17822699e-03\n",
        "   8.08196351e-03   7.88340260e-03   7.70056323e-03   7.64196265e-03\n",
        "   7.42925674e-03   7.35225814e-03   7.11202223e-03   7.06576286e-03\n",
        "   6.91462620e-03   6.84684491e-03   6.63475181e-03   6.46844401e-03\n",
        "   6.32196349e-03   6.25702627e-03   6.14168054e-03   5.99118663e-03\n",
        "   5.92316288e-03   5.81629495e-03   5.65989183e-03   5.34314317e-03\n",
        "   5.15085198e-03   4.94110984e-32   1.53127016e-32   8.35454279e-33]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from the scree plot we choose to pick the first 12 principal components\n",
      "pca12 = PCA(n_components=12, whiten=True) \n",
      "pca12.fit(X_train)\n",
      "\n",
      "# apply dimensionality reduction to the training set and the test set\n",
      "X_pca_train = pca12.transform(X_train)\n",
      "X_pca_test = pca12.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Stratified K-fold Cross Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the stratified K-fold cross validation, all the folds have size trunc(n_samples / n_folds). Each fold contains roughly the same proportions of the two types of class labels. This cross validation provides train/test indices to split data in train test sets, so we can evaluate the classifier performance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Stratified K-fold CV\n",
      "skf = cv.StratifiedKFold(y, n_folds=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVM classifier (RBF kernel)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The SVM classifier performance depends on the chosen kernel function and the parameters C and gamma.\n",
      "The **gamma** parameter defines how far the influence of a single training example reaches, with low values meaning \"far\" and high values meaning \"close\". **C** is a regularization parameter which trades off misclassification of training examples against simplicity of the decision surface. A large C makes constraints hard to ignore -> narrow separation margin. See here: http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html.\n",
      "\n",
      "In order to find the best set of parameters, we can perform a **grid search**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grid Search, first pass: coarse tuning of the parameters C and gamma\n",
      "C_range = 10.0 ** np.arange(7, 9) # 5, 10\n",
      "gamma_range = 10.0 ** np.arange(-5, 0) # -5, 0\n",
      "params = dict(gamma = gamma_range, C = C_range)\n",
      "classifier = svm.SVC(kernel='rbf')\n",
      "# SVM classifier optimized through cross validation\n",
      "clf = grid_search.GridSearchCV(classifier, param_grid = params, cv = skf)\n",
      "# fit the model after the reduced dimensionality performed by PCA\n",
      "clf.fit(X_pca_train, y)\n",
      "print(\"First pass: the best classifier is: \", clf.best_estimator_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('First pass: the best classifier is: ', SVC(C=10000000.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
        "  gamma=0.10000000000000001, kernel='rbf', max_iter=-1, probability=False,\n",
        "  random_state=None, shrinking=True, tol=0.001, verbose=False))\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grid Search, second pass: fine tuning of the parameters C and gamma\n",
      "C_range = 10.0 ** np.arange(6.5, 7.5, 0.1) \n",
      "gamma_range = 10.0 ** np.arange(-1.5, 0.5, 0.1) \n",
      "params = dict(gamma = gamma_range, C = C_range)\n",
      "classifier = svm.SVC(kernel='rbf')\n",
      "clf = grid_search.GridSearchCV(classifier, param_grid = params, cv = skf)\n",
      "clf.fit(X_pca_train, y)\n",
      "print(\"Second pass: the best classifier is: \", clf.best_estimator_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Second pass: the best classifier is: ', SVC(C=3162277.6601683791, cache_size=200, class_weight=None, coef0=0.0,\n",
        "  degree=3, gamma=0.31622776601683861, kernel='rbf', max_iter=-1,\n",
        "  probability=False, random_state=None, shrinking=True, tol=0.001,\n",
        "  verbose=False))\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = clf.best_estimator_.predict(X_pca_test)\n",
      "print(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 0 1 ..., 1 0 1]\n"
       ]
      }
     ],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}